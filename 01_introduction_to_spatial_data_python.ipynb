{"cells":[{"cell_type":"markdown","metadata":{"id":"zfOrevwdleMg"},"source":["# Introduction to spatial and spatiotemporal data in Python\n","\n","This session will be used to go over the basics of accessing and manipulating spatial data with Python, using standard tools in the current Python geospatial ecosystem:\n"," - `rasterio` - for reading/writing raster data (among other things)\n"," - `numpy` - for manipulating N-dimensional arrays\n"," - `geopandas` - for reading/writing vector data and manipulating them in the form of a `pandas.DataFrame` with added functionality\n","\n","Additionally, we will introduce some ease-of-use and performance oriented functionality implemented within `eumap`.\n","\n","First, let's install the packages we need that are not included in Colab."]},{"cell_type":"code","source":["!pip install rasterio shapely geopandas pygeos matplotlib==3.4\n","!pip install --no-deps --upgrade 'git+https://gitlab.com/geoharmonizer_inea/eumap.git#egg=eumap'"],"metadata":{"id":"C9xYc9n7lxYR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Basics: raster manipulation\n","\n","Let's create a geospatial raster dataset from scratch, starting with some arbitrary data."],"metadata":{"id":"wzSWMz9xQlIC"}},{"cell_type":"code","source":["import numpy as np\n","\n","data_range = np.linspace(-1, 1, 100)\n","\n","data, __ = np.meshgrid(data_range, data_range)\n","\n","print(data.shape)\n","data"],"metadata":{"id":"kopEaUdOVeMW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can visualize the data with `eumap.plotter`, which provides a wrapper around `matplotlib` that ensures image aspect ratio is preserved and provides some added functionality, like transparency on `nodata`."],"metadata":{"id":"M2Npu_avWP4R"}},{"cell_type":"code","source":["from eumap import plotter\n","\n","# in case of plots not appearing properly in Jupyter\n","%matplotlib inline\n","\n","plotter.plot_rasters(data, figsize=5)"],"metadata":{"id":"lKS73JxeWOxH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["To position our data in a geospatial context we need:\n","  1. a coordinate system (CRS)\n","  2. the position of our pixels within the CRS (a transform)\n","\n","Let's say this data is centered on the Equator and the Greenwich Meridian and has a spatial resolution of 1Â°, and then position it that way on the WGS84 ellipsoid."],"metadata":{"id":"C9T0YF-BXaaW"}},{"cell_type":"code","source":["crs = 'EPSG:4326'\n","\n","resolution = 1\n","center_x = -50\n","center_y = 50\n","\n","transform = [\n","    resolution,\n","    0,\n","    center_x,\n","    0,\n","    -resolution,\n","    center_y,\n","]"],"metadata":{"id":"h0ijlYopZqFa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","We can now use `rasterio`, a library that uses `GDAL` under the hood but provides an interface closer to idiomatic Python, to save the data to a GeoTiff. The positioning information, along with some other properties of the raster are all part of a `rasterio` dataset reader or writer object's `profile` property."],"metadata":{"id":"qvP1dW4Ta5i1"}},{"cell_type":"code","source":["import rasterio as rio\n","\n","raster_path = 'raster.tif'\n","\n","height, width = data.shape\n","\n","affine = rio.Affine(*transform)\n","\n","dst = rio.open(\n","    raster_path, 'w',\n","    driver='GTiff',                   # we have to specify a driver...\n","    width=width,                      # ...width...\n","    height=height,                    # ...height...\n","    count=1,                          # ...number of bands...\n","    dtype='float32',                  # ...data type...\n","    transform=affine,                 # ...transform is optional but the default one is not very useful...\n","    crs=crs,                          # ...same with CRS.\n","    nodata=-9999,                     # it is also useful to specify a nodata value which you know should not occur in your dataset\n",")\n","\n","profile = dst.profile\n","print(profile)\n","\n","dst.write(\n","    data, # the data we are writing\n","    1,    # the bands we are writing it to\n",")\n","\n","dst.close()"],"metadata":{"id":"eS4FHO07bsit"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Note that `rasterio` readers and writers can also be used as context managers (as one would use `open()` from the Python standard library).\n","\n","Let's check what was actually written."],"metadata":{"id":"iXnplY2hdTO9"}},{"cell_type":"code","source":["with rio.open(raster_path) as src:\n","    print(src.profile)\n","    data = src.read(1)\n","\n","plotter.plot_rasters(data, figsize=5)"],"metadata":{"id":"CQ0YaOojdWGI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now let's do some calculation with rasters. We can do that as we would with any numpy array."],"metadata":{"id":"r9_mBxrEkiND"}},{"cell_type":"code","source":["data1 = np.sin((data+.5)*np.pi)\n","data2 = np.cos(data.T*np.pi)\n","\n","data3 = (data1 + data2) * .5\n","\n","plotter.plot_rasters(data3, figsize=5)"],"metadata":{"id":"94xbOzFakoRB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can also save the file again, copy it to Drive and open it in QGIS to check if our positioning is correct."],"metadata":{"id":"cr_O-7f-j8_l"}},{"cell_type":"code","source":["from google.colab import drive\n","import shutil\n","\n","with rio.open(\n","    raster_path, 'w',\n","    **profile,\n",") as dst:\n","    dst.write(data3, 1)\n","\n","drive.mount('/content/drive')\n","shutil.copy(raster_path, '/content/drive/MyDrive')"],"metadata":{"id":"-s8kuXXyi7aZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Basics: vector manipulation\n","\n","For vector reading/writing and manipulation, we will use `geopandas` and `shapely`, a library that extends dataframes with capabilities for processing geometries in a vectorized manner. `geopandas` internally uses `fiona` for I/O and `shapely` (and more recently `pygeos`) for handling geometry primitives.\n","\n","Let's create some arbitrary geometries."],"metadata":{"id":"y8RDEBN1o4r5"}},{"cell_type":"code","source":["import shapely.geometry as g\n","\n","point1 = g.Point(-50, -50)\n","point2 = g.Point(-50, 50)\n","\n","g.MultiPoint([\n","    point1,\n","    point2,\n","])\n"],"metadata":{"id":"4ptR071YpUAG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["poly1 = g.box(25, 25, 75, 75)\n","poly2 = point1.buffer(25, resolution=1)\n","poly3 = point2.buffer(50, resolution=4)\n","\n","multipoly = g.MultiPolygon([\n","    poly1,\n","    poly2,\n","    poly3\n","])\n","\n","multipoly"],"metadata":{"id":"jatGOTdJqQu5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can create a `geopandas.GeoDataFrame` with these geometries, and use it to save them to a file."],"metadata":{"id":"_vtS9SoEtZqv"}},{"cell_type":"code","source":["import geopandas as gp\n","\n","gdf = gp.GeoDataFrame(geometry=[\n","    poly1,\n","    poly2,\n","    poly3,\n","])\n","\n","print(gdf)\n","\n","gdf.unary_union"],"metadata":{"id":"wmUBo1GAtn-L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["geometry_path = 'geometries.geojson'\n","\n","gdf.to_file(\n","  geometry_path,\n","  driver='GeoJSON',\n",")"],"metadata":{"id":"DQRXRqZht_HB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's see how the polygons overlap with our rasters."],"metadata":{"id":"BDhU821mq2wF"}},{"cell_type":"code","source":["with rio.open(raster_path) as src:\n","    data_extent = g.box(*src.bounds)\n","\n","g.MultiPolygon([\n","    poly1,\n","    poly2,\n","    poly3,\n","    data_extent,\n","])"],"metadata":{"id":"muBBaH9nq82g"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["These polygons intersect our raster data but are outside the bounds of it, let's clip the multipolygon."],"metadata":{"id":"TZQMg8QwsPi7"}},{"cell_type":"code","source":["multipoly_clipped = data_extent.intersection(multipoly)\n","\n","multipoly_clipped"],"metadata":{"id":"PNTTuzy2scIV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We could also clip each geometry individually, which is more comfortable to do with `geopandas`."],"metadata":{"id":"h81gGF4gtPW1"}},{"cell_type":"code","source":["gdf_clipped = gdf.copy()\n","gdf_clipped.geometry = gdf.intersection(data_extent)\n","\n","print(gdf_clipped)\n","\n","gdf_clipped.unary_union"],"metadata":{"id":"_De2EFDutMX8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can use the geometries we made to clip our raster data using `rasterio`. We will do this by creating a raster mask of the geometries with the same geotransform as the data."],"metadata":{"id":"fWgX6XqNvcbJ"}},{"cell_type":"code","source":["from rasterio import features\n","\n","geometry_mapping = g.mapping(multipoly)\n","\n","geometry_mapping"],"metadata":{"id":"zfI33WtEvxWM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mask = features.rasterize(\n","    [geometry_mapping],\n","    out_shape=data3.shape,\n","    transform=affine,\n",")\n","\n","print(mask)\n","\n","plotter.plot_rasters(mask, figsize=5, cmaps='Greys')"],"metadata":{"id":"7FkS26cPwPFe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["nodata = profile['nodata']\n","nodata_index = mask.astype(bool)\n","\n","data4 = data3.copy()\n","data4[nodata_index] = nodata\n","\n","plotter.plot_rasters(data4, figsize=5, nodata=nodata, vmin=data3.min(), vmax=data3.max())"],"metadata":{"id":"8-VCGYcXxkVz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Again we can export the data and view it."],"metadata":{"id":"z3Jrjlb3zjCp"}},{"cell_type":"code","source":["\n","with rio.open(\n","    raster_path, 'w',\n","    **profile,\n",") as dst:\n","    dst.write(data4, 1)\n","\n","shutil.copy(raster_path, '/content/drive/MyDrive')"],"metadata":{"id":"8ljSlIYYznXS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Some actual raster data\n","\n","Now, let's download some useful data. We will download some Landsat ARD samples from the [`eumap` benchmark dataset](https://zenodo.org/record/4311598) using the downloader provided in the library."],"metadata":{"id":"WXipeXg4m0_O"}},{"cell_type":"code","source":["from eumap.datasets import pilot\n","\n","pilot.get_datasets('greece')"],"metadata":{"id":"AuLLuZRdp3Pz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pilot.get_data('5606_greece_rasters.tar.gz')"],"metadata":{"id":"dK4SUS-NqNTI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This will take a couple of minutes so let's use this time for questions!\n","\n","When the data is downloaded we can again inspect the rasters with `rasterio`. Let's gather some raster file paths with a helper from `eumap`"],"metadata":{"id":"Tnkw_Aezlvh3"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"vmAUyScEleMj"},"outputs":[],"source":["\n","from pathlib import Path\n","from eumap.misc import find_files\n","\n","data_home = Path('eumap_data')\n","\n","tile_id = '5606_greece'\n","\n","tile_dir = data_home / tile_id\n","\n","raster_paths = find_files(tile_dir, '*.tif')\n","\n","print('N files:', len(raster_paths))\n","print()\n","for rpath  in raster_paths[:10]:\n","    print(rpath)"]},{"cell_type":"markdown","metadata":{"id":"4VZ6E-73leMn"},"source":["We will open the first file on the list. Again we get a `rasterio.DatasetReader` object."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jzVKKszSleMo"},"outputs":[],"source":["\n","raster = rio.open(raster_paths[0])\n","print(raster)"]},{"cell_type":"markdown","metadata":{"id":"iy9y5TNDleMq"},"source":["We can now inspect certain properties our real-world data..."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K7Z4zpiKleMq"},"outputs":[],"source":["\n","print('driver:', raster.driver)\n","print('N bands:', raster.count)\n","print('shape:', raster.shape)\n","print('CRS:', raster.crs)\n","print('transform:', raster.transform)\n","print('nodata:', raster.nodata)"]},{"cell_type":"markdown","metadata":{"id":"GAFKqIZvleMs"},"source":["...or just print the entire `profile` property."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ad_tdXhFleMt"},"outputs":[],"source":["\n","print(raster.profile)"]},{"cell_type":"markdown","metadata":{"id":"xN_TgqQkleMu"},"source":["Again, calling the `read()` method with the band index as the argument (starting from 1) provides us with a `numpy` array containing the data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jL0siWmfleMw"},"outputs":[],"source":["\n","data = raster.read(1)\n","print(data, type(data))"]},{"cell_type":"markdown","metadata":{"id":"wAhwyZRgleMx"},"source":["As our real-world data is also read into a regular `numpy.ndarray`, we can perform the same operations as with any other `numpy` array, like computing statistics with the array's methods."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jjrC47wZleMy"},"outputs":[],"source":["\n","data_min = data.min()\n","data_max = data.max()\n","data_median = np.median(data)\n","\n","nodata = raster.nodata\n","\n","plotter.plot_rasters(\n","    data,\n","    titles=f'min: {data_min}, max: {data_max}, median: {data_median}',\n","    figsize=5,\n","    nodata=nodata,\n","    vmin=data_min,\n","    vmax=data_max,\n",")"]},{"cell_type":"markdown","metadata":{"id":"_GmAS5cNleMz"},"source":["Aside from providing an easy way to deal with arrays and access to linear algebra, `numpy` supplies highly performant vectorized operations using BLAS libraries like MKL and OpenBLAS under the hood.\n","\n","Let's find out where our data falls outside the interval from the 20th to the 80th percentile. Comparing an array with a number (or another array with compatible dimensions) yields an array of boolean elements. We can perform operations on these boolean arrays with Python's bitwise boolean operators."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rbj7Q3FtleMz"},"outputs":[],"source":["\n","low, high = np.percentile(data, [20, 80])\n","print('P5:', low)\n","print('P95:', high)\n","\n","hi_index = data > high\n","lo_index = data < low\n","nodata_index = hi_index | lo_index\n","print('index:', nodata_index)\n","\n","pct_outside = 100 * nodata_index.sum() / nodata_index.size\n","print('% of data outside of bounds:', pct_outside.round(2))"]},{"cell_type":"markdown","metadata":{"id":"Z88ZbhFhleM0"},"source":["Using the index we can now alter the out-of-bounds pixels to the nodata value."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"njieKDpKleM0"},"outputs":[],"source":["\n","new_data = data.copy()\n","new_data[nodata_index] = nodata\n","\n","plotter.plot_rasters(\n","    new_data,\n","    titles=f'nodata between 20th and 80th percentile',\n","    figsize=5,\n","    nodata=nodata,\n","    vmin=int(data_min),\n","    vmax=int(data_max),\n",")"]},{"cell_type":"markdown","metadata":{"id":"1nzU_hnYleM1"},"source":["...or clip the data to the interval."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ujiSp5-hleM1"},"outputs":[],"source":["\n","new_data[lo_index] = low\n","new_data[hi_index] = high\n","\n","plotter.plot_rasters(\n","    new_data,\n","    titles=f'data clipped between 20th and 80th percentile',\n","    figsize=5,\n","    nodata=nodata,\n","    vmin=int(data_min),\n","    vmax=int(data_max),\n",")"]},{"cell_type":"markdown","metadata":{"id":"p9lHEEASleM1"},"source":["Notice that the areas previously containing no data are now filled with valid values. That's because we didn't account for that nodata mask. We can produce an index by either comparing the data array to the nodata value, or better yet, using the `read_masks()` method of the `DatasetReader`. While nodata masks can sometimes be accounted for later on, it is beneficial to conserve resources by avoiding computation on nodata altogether."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jrQMOZ_xleM2"},"outputs":[],"source":["\n","data_mask = raster.read_masks(1).astype(bool)\n","data_only = data[data_mask].copy()\n","\n","hi_index = data_only > high\n","lo_index = data_only < low\n","\n","data_only[lo_index] = low\n","data_only[hi_index] = high\n","\n","new_data[:] = nodata\n","new_data[data_mask] = data_only\n","\n","plotter.plot_rasters(\n","    new_data,\n","    titles=f'data clipped between 20th and 80th percentile',\n","    figsize=5,\n","    nodata=nodata,\n","    vmin=int(data_min),\n","    vmax=int(data_max),\n",")"]},{"cell_type":"markdown","metadata":{"id":"QEp63IKLleM2"},"source":["Now let's open a folder and write our new data to a file. Again, to open a file in write mode, we need to provide additional arguments, such as raster width and height in pixels, driver, etc. A transformation matrix and CRS are also required if we want our raster to be properly geospatially referenced. Again, as it often is with real-world examples, all of this is contained in the `profile` `dict` of our `DatasetReader`. Since we have not changed any properties of the raster other than the data itself, we can pass the entire `profile` to the `DatasetWriter` as `**kwargs`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4IKFYfjnleM2"},"outputs":[],"source":["import os\n","\n","out_dir = data_home/'session_1_outputs'\n","os.makedirs(out_dir, exist_ok=True)\n","\n","out_path = out_dir/'raster.tif'\n","\n","with rio.open(out_path, 'w', **raster.profile) as dst:\n","    print(dst)\n","    dst.write(new_data, 1)"]},{"cell_type":"markdown","metadata":{"id":"X-7_3WO4leM3"},"source":["`plotter.plot_rasters()` can also be called with dataset filepaths instead of data arrays. When used this way we do not have to provide the `nodata` argument, as it will be read from the file automatically."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FIY2DZGrleM3"},"outputs":[],"source":["\n","plotter.plot_rasters(\n","    out_path,\n","    figsize=5,\n","    vmin=int(data_min),\n","    vmax=int(data_max),\n",")"]},{"cell_type":"markdown","metadata":{"id":"-qzJ0tOGleM3"},"source":["## Some actual vector data\n","\n","Let's download and load the sample land cover points from the benchmark dataset and inspect the data."]},{"cell_type":"code","source":["point_datasets = pilot.get_datasets('landcover_samples')\n","point_datasets"],"metadata":{"id":"XXIMQhYE8VmY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pilot.get_data(point_datasets)"],"metadata":{"id":"j_aURo8U9Lzx"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KTX9LieLleM3"},"outputs":[],"source":["\n","points = gp.read_file(tile_dir/f'{tile_id}_landcover_samples.gpkg')\n","\n","crs = points.crs\n","\n","print('CRS:', crs)\n","print('N points:', points.index.size)\n","\n","points"]},{"cell_type":"markdown","source":["We can also read all of the geopackages we've downloaded and merge them:"],"metadata":{"id":"hy6kv2R5Bgmi"}},{"cell_type":"code","source":["point_files = [*data_home.glob('*/*_landcover_samples.gpkg')]\n","point_files"],"metadata":{"id":"mc6hIaABFk5m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","\n","points = pd.concat(map(\n","    gp.read_file,\n","    point_files,\n","))\n","\n","points = gp.GeoDataFrame(points, crs=crs)\n","\n","print('N points:', points.index.size)\n","\n","points"],"metadata":{"id":"7PSa3cUrBvNO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L1RCbatcleM4"},"source":["We can compare the extent of the points with that of our sample raster. Again, since `shapely` interacts nicely with Jupyter and visualizes geometries on output, we can use it to construct polygons from the bounding box coordinates of both datasets. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7ZE3xyRvleM4"},"outputs":[],"source":["\n","raster_extent = g.box(*raster.bounds)\n","print('raster extent:', raster_extent)\n","\n","points_bounds = points.cascaded_union.bounds\n","points_extent = g.box(*points_bounds)\n","print('points extent:', points_extent)\n","\n","g.MultiPolygon([raster_extent, points_extent])"]},{"cell_type":"markdown","metadata":{"id":"ANISzpanleM4"},"source":["We can see the extent of the points is much larger than that of the raster, so we will utilize `geopandas` to perform a vectorized intersection check over the points. This produces a boolean array which we can use to index only the points inside the raster bounds.\n","\n","While this is obviously not particularly useful in this case since we already had the subset in a separate file, it presents an example of an often encountered real-world situation."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c5idDYgmleM4"},"outputs":[],"source":["\n","point_subset_index = points.intersects(raster_extent)\n","\n","point_subset = points[point_subset_index]\n","\n","print('N points:', point_subset.index.size)\n","point_subset.cascaded_union"]},{"cell_type":"markdown","metadata":{"id":"KF85nPk1leM4"},"source":["We can now write our reduced point dataset to a file. Like `rasterio` (or `GDAL`), `geopandas` has support for various drivers. Here we will output the points to GeoJSON."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uFyVPz1qleM5"},"outputs":[],"source":["\n","import fiona\n","\n","#See https://github.com/Toblerity/Fiona/issues/977\n","with fiona.Env(OSR_WKT_FORMAT=\"WKT2_2018\"):\n","    point_subset.to_file(\n","        out_dir/'clipped_points.geojson',\n","        driver='GeoJSON',\n","    )"]},{"cell_type":"markdown","metadata":{"id":"8HKBeRH0leM5"},"source":["## Computing a raster time series\n","\n","Let's compute a spring NDVI timeseries for our tile from LANDSAT composites for the spring season for years 2000 to 2020. We can use `eumap.raster.read_rasters()` for a multithreaded read of multiple datasets into a single array. `read_rasters()` behaves in a time series friendly manner, stacking all layers into a multiband image. It also takes care of nodata masks, filling them with NaN."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dLeHis9kleM5"},"outputs":[],"source":["\n","from eumap.raster import read_rasters\n","\n","red_files = find_files(tile_dir, '*/landsat_ard_summer_red_p50.tif')\n","nir_files = find_files(tile_dir, '*/landsat_ard_summer_nir_p50.tif')\n","\n","red, __ = read_rasters(raster_files=red_files)\n","nir, __ = read_rasters(raster_files=nir_files)\n","\n","print('array shapes:', red.shape, nir.shape)"]},{"cell_type":"markdown","metadata":{"id":"DzPDpamBleM5"},"source":["We can now use the stacked data to compute the entire NDVI series at once. We will then plot the series at a single pixel."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2-spvKeqleM5"},"outputs":[],"source":["\n","import matplotlib.pyplot as plt\n","plt.style.use('seaborn')\n","\n","ndvi = (nir - red) / (nir + red)\n","\n","years = [*range(2000, 2020)]\n","\n","def plot_series(data, index):\n","    xi, yi = index\n","    fig, ax = plt.subplots()\n","    ax.plot(years, data[yi,xi,:])\n","    ax.set_title(f'NDVI series at pixel {index}')\n","    ax.set_xticklabels(years, rotation=90)\n","\n","plot_series(ndvi, (500, 500))"]},{"cell_type":"markdown","metadata":{"id":"e6H_Vdr0leM6"},"source":["We can also use `plotter` to plot the series over the entire tile, but first we have to unstack the image into separate arrays for each year."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2B5vN0MIleM6"},"outputs":[],"source":["\n","ndvi = np.moveaxis(ndvi, -1, 0)\n","ndvi.shape"]},{"cell_type":"markdown","metadata":{"id":"BZZZZzICleM6"},"source":["We will now plot the series over the last five years."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7to-r9oileM6"},"outputs":[],"source":["\n","plotter.plot_rasters(\n","    *ndvi.astype(np.float32)[-5:],\n","    figsize=10,\n","    titles=years[-5:],\n","    cmaps='Greens',\n","    vmin=-1,\n","    vmax=1,\n",")"]},{"cell_type":"markdown","metadata":{"id":"lZppq1HwleM6"},"source":["Let's compute for each year the NDVI difference to the previous one and plot the results for the last five years."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pV72cZRkleM7"},"outputs":[],"source":["\n","diff = ndvi[1:] - ndvi[:-1]\n","\n","plotter.plot_rasters(\n","    *diff.astype(np.float32)[-5:],\n","    figsize=10,\n","    titles=years[-5:],\n","    cmaps='RdYlGn',\n","    vmin=-.5,\n","    vmax=.5,\n",")"]},{"cell_type":"markdown","metadata":{"id":"p8OBsb4FleM7"},"source":["We can use `save_rasters()` to batch write results for all the years in parallel, analogue to `read_rasters()`. `save_rasters()` takes series as multiband images, so we have to stack our `diff` array."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-V81Ue5RleM7"},"outputs":[],"source":["\n","from eumap.raster import save_rasters\n","\n","out_files = [out_dir/f'ndvi_diff/ndvi_diff_{year}.tif' for year in years[1:]]\n","\n","save_rasters(\n","    raster.name,\n","    out_files,\n","    np.stack(diff, -1).astype(np.float32),\n","    dtype='float32',\n","    nodata=-99.,\n",")"]},{"cell_type":"markdown","metadata":{"id":"nSKN6T8rleM7"},"source":["Let's overlay the results with our point subset. `eumap` provides a parallelized way of overlaying points with rasters."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AljB_vz6leM7"},"outputs":[],"source":["\n","from eumap.mapper import SpaceOverlay\n","from datetime import datetime\n","overlay_points = point_subset\n","\n","overlay = SpaceOverlay(\n","    point_subset[['geometry']],\n","    out_files,\n",").run()"]},{"cell_type":"markdown","metadata":{"id":"854JQMM6leM8"},"source":["Each point will now have attached the time series of NDVI differences."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kLtO4LoVleM8"},"outputs":[],"source":["overlay"]},{"cell_type":"markdown","metadata":{"id":"QMwkx3rqleM8"},"source":["Let's plot the series at one of the points."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AFoMK8ZXleM8"},"outputs":[],"source":["\n","def plot_series(point_id):\n","    diff_cols = sorted(overlay.columns[2:])\n","    fig, ax = plt.subplots()\n","    series = overlay.loc[point_id][diff_cols].values\n","    ax.plot(years[1:], series)\n","    ax.set_title(f'NDVI compared to previous year at point {point_id}')\n","    ax.set_xticklabels(years[1:], rotation=90)\n","\n","plot_series(800)"]},{"cell_type":"markdown","metadata":{"id":"1azSa59cleM8"},"source":["## Raster block processing\n","\n","`rasterio` allows for rasters to be read from within a defined window."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nE8vC3ExleM9"},"outputs":[],"source":["\n","from rasterio.windows import Window\n","\n","window = Window(\n","    col_off=0,\n","    row_off=0,\n","    width=5,\n","    height=5,\n",")\n","\n","window_data = raster.read(1, window=window)\n","\n","window_data"]},{"cell_type":"markdown","metadata":{"id":"AXGznTvyleM9"},"source":["Cloud optimized GeoTIFFs are internally organized into blocks with local compression. Let's check the block number of our raster."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CewzcTgPleM9"},"outputs":[],"source":["\n","block_windows = [*raster.block_windows()]\n","\n","print('N blocks:', len(block_windows))"]},{"cell_type":"markdown","metadata":{"id":"S-qu1pVTleM9"},"source":["When reading a window from a raster, all blocks intersecting the window have to be read and decompressed. So if we read a single raster in parallel but only at windows corresponding to block boundaries, we minimize read time. `eumap.parallel.blocks` leverages this to enable efficient processing of large datasets, which allows for both faster processing on large hardware infrastructure and for long-running processing with limited resources.\n","\n","We will compute NDVI for a single season within the boundary of our raster, but this time from pan-european LANDSAT mosaics hosted in S3 buckets.\n","\n","To do this we first have to convert the geometry into the GeoJSON schema, as we did when we used `rasterio.features.rasterize` to create a geometry mask."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x1-zwmCQleM9"},"outputs":[],"source":["\n","red_url = 'http://s3.eu-central-1.wasabisys.com/eumap/landsat/landsat_ard_20180625_20180912_red_p50.tif'\n","nir_url = 'http://s3.eu-central-1.wasabisys.com/eumap/landsat/landsat_ard_20180625_20180912_nir_p50.tif'\n","\n","with rio.open(red_url) as src:\n","    print('raster size:', src.shape)\n","\n","geometry = g.mapping(raster_extent)\n","\n","print(geometry)"]},{"cell_type":"markdown","metadata":{"id":"b1LfZyllleM-"},"source":["We will now initialize the reader and writer and define NDVI as a function."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kcnZmlePleM-"},"outputs":[],"source":["\n","from eumap.parallel.blocks import RasterBlockReader, RasterBlockWriter\n","\n","def calc_ndvi(red, nir):\n","    return (nir - red) / (nir + red)\n","\n","reader = RasterBlockReader(reference_file=red_url)\n","writer = RasterBlockWriter(reader=reader)"]},{"cell_type":"markdown","metadata":{"id":"WQTu__L5leM-"},"source":["We can now start the block-wise processing and write the result to a file."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PPfwmUFVleM-"},"outputs":[],"source":["\n","out_file = out_dir/'ndvi_blocks.tif'\n","\n","writer.write(\n","    src_path=[red_url, nir_url],\n","    dst_path=out_file,\n","    geometry=geometry,\n","    block_func=calc_ndvi,\n","    nodata=-9999.,\n","    dtype='float32',\n",")"]},{"cell_type":"markdown","metadata":{"id":"KU02_NTqleM-"},"source":["Let's check the results."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"js-MD_hmleM_"},"outputs":[],"source":["\n","plotter.plot_rasters(\n","    out_file,\n","    cmaps='Greens',\n","    figsize=5,\n","    vmin=-1,\n","    vmax=1,\n",")"]},{"cell_type":"markdown","metadata":{"id":"x3yKydHnleM_"},"source":["## ODS data catalogue\n","\n","`eumap.datasets.Catalogue` provides some abstraction over the ODS data catalogue. It provides a search utility to access dataset URLs."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VmNjzatIleM_"},"outputs":[],"source":["\n","from eumap.datasets import Catalogue\n","\n","cat = Catalogue(use_csw=False)\n","results = cat.search('ndvi')\n","\n","results"]},{"cell_type":"markdown","metadata":{"id":"tFpNkbWsleM_"},"source":["We can ommit unwanted results with the `exclude` keyword argument"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XTR_qTKqleM_"},"outputs":[],"source":["\n","results = cat.search('ndvi', exclude=['trend'])\n","\n","results"]},{"cell_type":"markdown","metadata":{"id":"pVfZe6qEleM_"},"source":["...and also search by year."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M1dMAQbXleNA"},"outputs":[],"source":["\n","results = cat.search(\n","    'ndvi',\n","    exclude=['trend'],\n","    years=[2019]\n",")\n","\n","results"]},{"cell_type":"markdown","metadata":{"id":"PHQuf78EleNA"},"source":["Since the results behave more or less like regular Python strings, we can sort them into a time series and read with `read_rasters`. We will read the window corresponding to our test raster..."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v-jtaC4tleNA"},"outputs":[],"source":["from rasterio.windows import from_bounds\n","\n","results = sorted(results)\n","\n","ref = rio.open(results[0])\n","\n","window = from_bounds(\n","    *raster_extent.bounds,\n","    transform=ref.transform,\n",")\n","\n","q_ndvi, __ = read_rasters(\n","    raster_files=[*map(str, results)],\n","    spatial_win=window,\n","    dtype=ref.profile['dtype'],\n",")"]},{"cell_type":"markdown","metadata":{"id":"l_2O1S0yleNA"},"source":["...and plot the results."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YGG9pJzKleNA"},"outputs":[],"source":["\n","plotter.plot_rasters(\n","    *np.moveaxis(q_ndvi, -1, 0),\n","    figsize=5,\n","    cmaps='Greens',\n","    nodata=ref.nodata,\n","    vmin=int(q_ndvi.min()),\n","    vmax=int(q_ndvi.max()),\n",")\n"]},{"cell_type":"markdown","source":["## Bonus: creating a local Python environment\n","\n","An easy way to create a Python environment ready for geospatial computation is to use [Anaconda](https://www.anaconda.com/), or more conveniently the [miniforge](https://github.com/conda-forge/miniforge) distribution, which also optionally substitutes the `conda` manager with `mamba`, a much faster alternative, in distributions called `mambaforge`.\n","\n","To install `mambaforge` in a Linux environment you can run the following:\n","\n","```\n","# get the installer\n","wget https://github.com/conda-forge/miniforge/releases/latest/download/Mambaforge-Linux-x86_64.sh \n","\n","# run it to go through the installer\n","bash Mambaforge-Linux-x86_64.sh\n","\n","# initialize mamba\n","/path/to/your/install/directory/mambaforge/bin/mamba init\n","\n","# apply changes to .bashrc\n","source ~/.bashrc\n","```\n","\n","Now you can create an enviroment for geospatial computation. Let's create one that can reproduce the examples from this session and call it `gis37`.\n","\n","```\n","# create the environment\n","mamba create -n gis37 -c conda-forge python=3.7 numpy rasterio shapely geopandas matplotlib\n","\n","# activate it\n","mamba activate gis37\n","\n","# additionally install eumap (no conda package for it)\n","pip install --no-deps --upgrade 'git+https://gitlab.com/geoharmonizer_inea/eumap.git#egg=eumap'\n","```\n","\n","Regardless of the fact you can now create your own environment, we still strongly recommend you use Colab for the remainder of the Python track.\n","\n","Thank you for your attention!"],"metadata":{"id":"Ulh64ZYv-9gn"}}],"metadata":{"anaconda-cloud":{},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"colab":{"name":"01_introduction_to_spatial_data_python.ipynb","provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":0}